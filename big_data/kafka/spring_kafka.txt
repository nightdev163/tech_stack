4.1.3 Sending Messages
Q:什么是KafkaTemplate
A:The KafkaTemplate wraps a producer and provides convenience methods to send data to Kafka topics. The following listing shows the relevant methods from KafkaTemplate.


Q:producer 是哪个对象创建的
A:


Q:producer 数量如何控制
A:

4.1.4 Receiving Messages

KafkaMessageListenerContainer
	The KafkaMessageListenerContainer receives all message from all topics or partitions on a single thread. 
	

ConcurrentMessageListenerContainer
	The ConcurrentMessageListenerContainer delegates to one or more KafkaMessageListenerContainer instances to provide multi-threaded consumption.


ConsumerAwareRecordInterceptor, BatchInterceptor	// 拦截器
	 In addition, the ConsumerAwareRecordInterceptor (and BatchInterceptor) provide access to the Consumer<?, ?>. This might be used, for example, to access the consumer metrics in the interceptor.

RecordInterceptor ConsumerAwareRecordInterceptor BatchInterceptor 是 Container 里成员

Starting with version 2.2.4, you can specify Kafka consumer properties directly on the annotation, these will override any properties with the same name configured in the consumer factory. You cannot specify the group.id and client.id properties this way; they will be ignored; use the groupId and clientIdPrefix annotation properties for those.


The properties are specified as individual strings with the normal Java Properties file format: foo:bar, foo=bar, or foo bar.

@KafkaListener(topics = "myTopic", groupId = "group", properties = {
    "max.poll.interval.ms:60000",
    ConsumerConfig.MAX_POLL_RECORDS_CONFIG + "=100"
})

@KafkaListener(id = "one", topics = "one")
public void listen1(String in) {
    System.out.println("1: " + in);
}

@KafkaListener(id = "two", topics = "two",
        properties = "value.deserializer:org.apache.kafka.common.serialization.ByteArrayDeserializer")
public void listen2(byte[] in) {
    System.out.println("2: " + new String(in));
}



listener containers
The listener containers created for @KafkaListener annotations are not beans in the application context. Instead, they are registered with an infrastructure bean of type KafkaListenerEndpointRegistry. 

@KafkaListener(id = "multi", topics = "myTopic")
static class MultiListenerBean {

    @KafkaHandler
    public void listen(String foo) {
        ...
    }

    @KafkaHandler
    public void listen(Integer bar) {
        ...
    }

    @KafkaHandler(isDefault = true)
    public void listenDefault(Object object) {
        ...
    }

}

!!注意 listener中id属性, The unique identifier of the container for this listener. 不是listener id




ContainerProperties
	consumerRebalanceListener
	

Q: rebalance 是什么概念?


Q: revoke 是什么概念?



DefaultKafkaProducerFactory

Q: DefaultKafkaProducerFactory是什么类
A: a ProducerFactory is used to create the producer

Q: 内部有哪些工作细节
A:
creates a singleton producer 
	When not using Transactions, by default, the DefaultKafkaProducerFactory creates a singleton producer used by all clients, as recommended in the KafkaProducer javadocs. 
	However, if you call flush() on the template, this can cause delays for other threads using the same producer. 
create (and cache) a separate producer for each thread
	 DefaultKafkaProducerFactory has a new property producerPerThread. When set to true, the factory will create (and cache) a separate producer for each thread, to avoid this issue.	
垃圾清理
	when producerPerThread is true, user code must call closeThreadBoundProducer() on the factory when the producer is no longer needed. This will physically close the producer and remove it from the ThreadLocal. Calling reset() or destroy() will not clean up these producers.

Q:如何更新producer参数
Starting with version 2.5.10, you can now update the producer properties after the factory is created.The changes will not affect existing producer instances; call reset() to close any existing producers so that new producers will be created using the new properties.NOTE: You cannot change a transactional producer factory to non-transactional, and vice-versa.


ReplyingKafkaTemplate




Thread Safety

A:concurrent message listener container线程安全吗?

Q:When using a concurrent message listener container, a single listener instance is invoked on all consumer threads. 



A:how to gurantee thread safe?

Q:
Use n containers with concurrency=1 with a prototype scoped MessageListener bean so that each container gets its own instance (this is not possible when using @KafkaListener).

Keep the state in ThreadLocal<?> instances.

Have the singleton listener delegate to a bean that is declared in SimpleThreadScope (or a similar scope).

第一个方法最简单吧
第三个方法还要清理线程状态
To facilitate cleaning up thread state (for the second and third items in the preceding list), starting with version 2.2, the listener container publishes a ConsumerStoppedEvent when each thread exits. You can consume these events with an ApplicationListener or @EventListener method to remove ThreadLocal<?> instances or remove() thread-scoped beans from the scope. Note that SimpleThreadScope does not destroy beans that have a destruction interface (such as DisposableBean), so you should destroy() the instance yourself.


4.1.11 Monitorying
Q:MicrometerConsumerListener, 这个类是干啥的？
A:

Q:MicrometerConsumerListener 的成员 MeterRegistry 是干啥的
A:

Q:MicrometerConsumerListener 的成员 Map<String, KafkaClientMetrics> metrics 是干啥的
A:


Q:DefaultKafkaConsumerFactory如何添加 MicrometerConsumerListener
A:
DefaultKafkaConsumerFactory<String, String> cf = new DefaultKafkaConsumerFactory<>(configs);
cf.addListener(new MicrometerConsumerListener<String, String>(meterRegistry(),
            Collections.singletonList(new ImmutableTag("customTag", "customTagValue"))));


4.1.12. Transactions
Q:如何启用Transactions
A:需要提供DefaultKafkaProducerFactory 和 transactionIdPrefix


Q:DefaultKafkaProducerFactory(factory) 在Transactions 处理中是干嘛的?
A:. In that case, instead of managing a single shared Producer, the factory maintains a cache of transactional producers. When the user calls close() on a producer, it is returned to the cache for reuse instead of actually being closed.

Q:transactional.id 是如何构成的
A: The transactional.id property of each producer is transactionIdPrefix + n, where n starts with 0 and is incremented for each new producer, unless the transaction is started by a listener container with a record-based listener. In that case, the transactional.id is <transactionIdPrefix>.<group.id>.<topic>.<partition>. 

Q:什么是fencing zombies
A:https://www.confluent.io/blog/transactions-apache-kafka/

Q:如何在spring boot时间transaction practice
A:With Spring Boot, it is only necessary to set the spring.kafka.producer.transaction-id-prefix property - Boot will automatically configure a KafkaTransactionManager bean and wire it into the listener container.

Q:KafkaTransactionManager introduce
The KafkaTransactionManager is an implementation of Spring Framework’s PlatformTransactionManager. It is provided with a reference to the producer factory in its constructor. If you provide a custom producer factory, it must support transactions. See ProducerFactory.transactionCapable().

You can use the KafkaTransactionManager with normal Spring transaction support (@Transactional, TransactionTemplate, and others). If a transaction is active, any KafkaTemplate operations performed within the scope of the transaction use the transaction’s Producer. The manager commits or rolls back the transaction, depending on success or failure. You must configure the KafkaTemplate to use the same ProducerFactory as the transaction manager.



Transaction Synchronization
kafka transaction example
@Transactional
public void process(List<Thing> things) {
    things.forEach(thing -> this.kafkaTemplate.send("topic", thing));
    updateDb(things);
}
When the method exits, the database transaction will commit followed by the Kafka transaction. If you wish the commits to be performed in the reverse order (Kafka first), use nested @Transactional methods, with the outer method configured to use the DataSourceTransactionManager, and the inner method configured to use the KafkaTransactionManager.


Consumer-Initiated Transactions
use a KafkaTransactionManager in the container to start the Kafka transaction and annotate the listener method with @Transactional to start the other transaction.


KafkaTemplate Local Transactions
You can use the KafkaTemplate to execute a series of operations within a local transaction. The following example shows how to do so:
boolean result = template.executeInTransaction(t -> {
    t.sendDefault("thing1", "thing2");
    t.sendDefault("cat", "hat");
    return true;
});
The argument in the callback is the template itself (this). If the callback exits normally, the transaction is committed. If an exception is thrown, the transaction is rolled back.


transaction example

@Transactional
public void process(List<Thing> things) {
    things.forEach(thing -> this.kafkaTemplate.send("topic", thing));
    updateDb(things);
}
The interceptor for the @Transactional annotation starts the transaction and the KafkaTemplate will synchronize a transaction with that transaction manager; each send will participate in that transaction. When the method exits, the database transaction will commit followed by the Kafka transaction.t

The KafkaTemplate will synchronize its transaction with the DB transaction and the commit/rollback occurs after the database.
@Transactional("dstm")
public void someMethod(String in) {
    this.kafkaTemplate.send("topic2", in.toUpperCase());
    this.jdbcTemplate.execute("insert into mytable (data) values ('" + in + "')");
}

If you wish to commit the Kafka transaction first, and only commit the DB transaction if the Kafka transaction is successful, use nested @Transactional methods:
@Transactional("dstm")
public void someMethod(String in) {
    this.jdbcTemplate.execute("insert into mytable (data) values ('" + in + "')");
    sendToKafka(in);
}

@Transactional("kafkaTransactionManager")
public void sendToKafka(String in) {
    this.kafkaTemplate.send("topic2", in.toUpperCase());
}


4.1.13 Exactly Once Semantics

Q: what is Exactly Once Sematics (EOS)
A:This means that, for a read→process-write sequence, it is guaranteed that the sequence is completed exactly once. (The read and process are have at least once semantics).


4.1.14. Wiring Spring Beans into Producer/Consumer Interceptors

Q:what is interceptor
A:Apache Kafka provides a mechanism to add interceptors to producers and consumers. These objects are managed by Kafka, not Spring, and so normal Spring dependency injection won’t work for wiring in dependent Spring Beans. However, you can manually wire in those dependencies using the interceptor config() method. 


4.1.17. Serialization, Deserialization, and Message Conversion

It is present with the org.apache.kafka.common.serialization.Serializer<T> and org.apache.kafka.common.serialization.Deserializer<T> abstractions with some built-in implementations.

Q:如何设置序列化和反序列化对象
A: 
1 通过config添加
code example
@Bean
    public ConsumerFactory<?, ?> kafkaConsumerFactory(SomeBean someBean) {
        Map<String, Object> consumerProperties = new HashMap<>();
        consumerProperties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);
        consumerProperties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        return new DefaultKafkaConsumerFactory<>(consumerProperties);
    }
    
@Bean
    public ProducerFactory<?, ?> kafkaProducerFactory(SomeBean someBean) {
        Map<String, Object> producerProperties = new HashMap<>();
        producerProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);
        producerProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
	return new DefaultKafkaProducerFactory<>(producerProperties);
    } 

2 KafkaConsumer KafkaProducer 构造函数参数里可以设置

Q:实现一个基础的json序列化
A:
Spring for Apache Kafka also provides JsonSerializer and JsonDeserializer implementations that are based on the Jackson JSON object mapper. 



Q:什么是Spring Messaging？
	To let you easily convert to and from org.springframework.messaging.Message, Spring for Apache Kafka provides a MessageConverter abstraction with the MessagingMessageConverter implementation and its JsonMessageConverter (and subclasses) customization.



4.1.18 Message Headers
As of version 2.0, Spring for Apache Kafka now supports mapping these headers to and from spring-messaging MessageHeaders.


Q:inbound outbound是什么概念
A inbound: 从内存进入到kafka MessageHeaders
  outbound: 从MessageHeaders到ConsumerRecord



